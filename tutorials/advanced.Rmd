---
title: "advanced_methods"
author: "Nathan Fox"
date: "30/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Now lets look at some more advanced methods - if you haven't been able to follow along, we can create a break out room with one of us to help you go back over the basics, so please let us know!

## Computer vision

Computer vision is a machine learning approach to assessing what is in a photograph. These methods are particularly important for large data sets that it would not be possible to assess manually. It is possible to build your own machine learning model to assess images, but that is out of the scope of this introductory workshop, so instead we will take a look at a pre-made model. The Google Vision Cloud API provides a range of functionality, from being able to identifying objects in images, extracting color pallets and even identifying iconic structures or landscapes.

Lets use a photograph of some swans. Putting it into the online version of Google Vision it can segment the image and identify the location of the birds.You can see it has outlined the birds with a green box and is 81% and 78% confident that these are birds.
![](C:/Users/n_fox/Documents/code/iale_workshop/tutorial_figures/vision_a.PNG)
It also gives us information about what objects are in the photograph. These are provided in step-wise process, starting with broad objects and narrowing down to components (e.g., starting with tree, then identifying a branch).
![](C:/Users/n_fox/Documents/code/iale_workshop/tutorial_figures/vision_b.PNG)

Finally, it returns information on the color properties. 
![](C:/Users/n_fox/Documents/code/iale_workshop/tutorial_figures/vision_c.PNG)

### Running computer vision from R

Please note that these methods require a new API key and a paid Google Vision account so we will not run new analysis l live. But if you are interested in signing up you can check out the Google [website ](https://cloud.google.com/vision).

Lets walk through the process. First, as before lets library the needed R packages. Then you would need to set up r to talk to Google, this is done by saving your Google API and initiating a connection through the `Sys.setenv()` and `gvision_init()` functions.

```{r, eval = FALSE}
library(pacman)
p_load("imgrec")
p_load_gh("ropensci/photosearcher")

#set up gvision
Sys.setenv(gvision_key = "") #Your Google API Key
gvision_init()

#search google vision
results <- get_annotations(images = paste(img_ids$url_l), # image url
                           features = "label", # request all available features
                           max_res = 10, # maximum number of results per feature
                           mode = 'url') # determine image type

#parse results
temp_file_path <- tempfile(fileext = '.json')
save_json(results, temp_file_path)
img_data <- parse_annotations(results)

#df of gvision tags
gvision_results <- img_data$labels
write.csv(gvision_results, ".\\data\\hiking_additional.csv")
```

Next we would search Flickr (or Reddit) for what images we would like to assess. In this example, the aim is to see what people do while hiking. 

```{r, eval = FALSE}
flickr_vision <- photo_search(text = "hiking",
                            mindate_taken = "2022-01-01",
                            maxdate_taken = "2022-01-08")
```

Then we can use the URL of the images to tell Google to identify its contents. this is done using the `get_annotations()` function. The data returned is in "JSON" format, lets not go into that here, but the section of code labeled "parse results" are a set of functions that help turn the results from JSON into a more friendly data frame that we have previously been dealing with.

```{r, eval = FALSE}
#search google vision
results <- get_annotations(images = paste(flickr_vision$url_l), # image url
                           features = "label", # request all available features
                           max_res = 10, # maximum number of results per feature
                           mode = 'url') # determine image type

#parse results
temp_file_path <- tempfile(fileext = '.json')
save_json(results, temp_file_path)
img_data <- parse_annotations(results)
vision_results <- img_data$labels

#save the outputs
write.csv(vision_results, ".\\data\\gvision_example.csv")
```

Lets read what this would have created.

```{r}
vision_results <- read.csv("~\\code\\iale_workshop\\data\\gvision_example.csv")

head(vision_results)
```

## Text sentiment

Textual sentiment analysis assess the emotion expressed within a piece of text. This can be a powerful tool in understanding how people feel about the natural environment. As with previous task we need to download and  library the necersarry packages to perform these analysis.

```{r}
library(pacman)
p_load("tidytext",
       "textdata",
       "dplyr")

p_load_gh("ropensci/photosearcher")
```

The most basic textual sentiment analysis is performed by comparing words to a pre-defined dictionary. These dictionaries have been created by other researchers who manually attributed a value to the sentiment of individual word. Today we will look at three dictionaries AFINN, bing and ncr. 

First, the AFINN dictionary [(Nielsen 2011)](https://arxiv.org/abs/1103.2903) ranks words on a scale of -5 to +5. Words negatively ranked are those with a negative associated sentiment and those with a positive rank are associated with a positive sentiment. While the number represents the magnitude of sentiment (e.g., +5 is more positive than +3).

```{r}
get_sentiments("afinn")
```

Second, the bing dictionary [Liu et al. ](https://www.morganclaypool.com/doi/abs/10.2200/s00416ed1v01y201204hlt016?casa_token=zVb-dykzCngAAAAA:joawB4fnvH6TWALFJeKJS8HiQQ07g920cdnjogMvSesova-GyXExeT7wwFkW2C6XjppwyThDHA) ranks word in a binary fashion of either positive of negative.

```{r}
get_sentiments("bing")
```

Third, the ncr dictionary [Mohammad and Turney 2010,](https://arxiv.org/pdf/1308.6297.pdf) categories word into different emotions: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

```{r}
get_sentiments("nrc")
```

You may have noticed, not only do these dictionaries have different methods of measuring sentiment, they also have a different number of categorized words. This makes each dictionary good for different purposes. It has been demonstrated that AFINN is a good dictionary for evaluating social media data, so lets focus on that for now. 

### AFINN and Flickr text

Lets get some Flickr data.

```{r}
flickr_text <- photo_search(text = "mountain",
                            mindate_taken = "2022-01-01",
                            maxdate_taken = "2022-01-08")
```

Now lets mergre all the text associated with the post (title, tags and description) into a single column. As the Flickr data is messy and contains lots of missing data we first create our own unique function called `paste2()`. For now lets not worry about the details of this function, but by doing `paste2 <- function(){}` we are telling R to create a new function called `paste2()` that carries out any code in the `{}` on the data defined in the `()`. To create a new column we can just use the `$` followed by a name of a column that doesn't alread exist, and then tell R what to place in this column


```{r}
#function to paste without copying NAs
paste2 <- function(...,sep=", ") {
  L <- list(...)
  L <- lapply(L,function(x) {x[is.na(x)] <- ""; x})
  gsub(paste0("(^",sep,"|",sep,"$)"),"",
       gsub(paste0(sep,sep),sep,
            do.call(paste,c(L,list(sep=sep)))))
}

#here we create a new column called text and paste in all the other text from that row
flickr_text$text <- paste2(flickr_text$title, 
                           flickr_text$description, 
                           flickr_text$tags) 

#lets inspect
head(flickr_text$text)
```

Finally, we need to use the AFINN dictionary to assess the sentiment of each word in the new 'text' column. 


The easiest way to do this is separate each piece of text into the individual words on a new row. 

```{r}
#unnest words - one row per word
flickr_text <- flickr_text %>%
  unnest_tokens(word, text) #creates col called word which matches afinn dictionary

head(flickr_text$word)
```

Next we add the dictionary sentiment value as a new row next to it. As both our data and the AFINN dictionary have a column called 'word' we can match our words to the values in the dictionary. This is achieved through the  `inner_join(get_sentiments("afinn")`. We then need to sum all the values for each individual photograph back up.This is achieved by the following lines of code: `group_by(url_l) %>% summarise(sentiment = sum(value))`

In these parts of the code we are using `%>%` to tell R to remeber the first piece of data and apply the following functions to it in a step-by-step way. This is a bit complicated but all covered in a short amount of code, so please ask for clarification if needed!

```{r}
#df of sum sentiment per photo 
afinn <- flickr_text %>% #select our data
  inner_join(get_sentiments("afinn")) %>% #add the afinn values as a new column
  group_by(url_l) %>%
  summarise(sentiment = sum(value))

head(afinn)
```

And there we have it, a summaries of the expressed sentiment for each photograph uploaded of a mountain in the first week of January. We can combine this with the spatial element or combine it with the image contents to assess the drivers of these sentiments. 
